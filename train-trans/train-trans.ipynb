{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e8dbe87",
   "metadata": {
    "cellId": "dsjmeuiu5ooqr5um2elwq",
    "execution_id": "63041a91-be5f-4ca8-9fcc-ca840d96b49c"
   },
   "source": [
    "## Обучение генеративной трансформерной модели с помощью `transformers`\n",
    "\n",
    "В этой работе мы познакомимся на практике с процессом тренировки большой трансформерной языковой модели. Поскольку такая тренировка требует существенных вычислительных ресурсов, выполнять эту работу рекомендуется в Yandex DataSphere, в которой доступны вычислитльные узлы с одни или двумя графическими процессорами Tesla V100.\n",
    "\n",
    "### Архитектура трансформеров\n",
    "\n",
    "В рамках этой работы мы предполагаем, что вы уже знакомы с архитектурой трансформеров, например, по [статье из ML-хэндбука](https://academy.yandex.ru/handbook/ml/article/transformery). Также для первоначального знакомства рекомендую заметку [Jay Alammar. The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/), и её частичный [русскоязычный перевод](https://habr.com/ru/articles/486358/).\n",
    "\n",
    "Мы не будем в рамках работы создавать архитетуру нейросети \"с нуля\". Если вам инетересно изучить реализацию трансформеров - рекомендую посмотреть на [NanoGPT](https://github.com/karpathy/nanoGPT). Подробно эта реализация разбирается в [этом видео](https://www.youtube.com/watch?v=kCc8FmEb1nY).\n",
    "\n",
    "### Библиотека `transformers` и её друзья\n",
    "\n",
    "Стандартом де факто в реализации трансформеров служит библиотека `transformers` от [HuggingFace](http://huggingface.co). Она содержит в себе реализацию большого количества используемых трансформерных архитектур, а также ряд полезных инструментов для их обучения. Многие инструменты также оформлены в виде отдельных библиотек, которые хорошо работают вместе:\n",
    "\n",
    "* `tokenizers` - быстрая реализация различных токенизаторов, позволяющих разделять входной текст на токены\n",
    "* `datasets` - манипулирование большими датасетами\n",
    "* `evaluate` - вычисление различных метрик и оценка результатов обучения\n",
    "* `accelerate` - реализация вычислений на множестве GPU и на вычислительных кластерах\n",
    "\n",
    "Для начала, установим необходимые библиотеки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac5909a2",
   "metadata": {
    "cellId": "rbd025oj0my1im4ob4369",
    "execution": {
     "iopub.execute_input": "2023-11-22T13:06:06.004559Z",
     "iopub.status.busy": "2023-11-22T13:06:06.003903Z",
     "iopub.status.idle": "2023-11-22T13:06:17.327182Z",
     "shell.execute_reply": "2023-11-22T13:06:17.325725Z",
     "shell.execute_reply.started": "2023-11-22T13:06:06.004524Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/jupyter/.local/lib/python3.10/site-packages (4.34.1)\n",
      "Requirement already satisfied: tokenizers in /home/jupyter/.local/lib/python3.10/site-packages (0.14.1)\n",
      "Requirement already satisfied: datasets in /home/jupyter/.local/lib/python3.10/site-packages (2.14.6)\n",
      "Requirement already satisfied: evaluate in /home/jupyter/.local/lib/python3.10/site-packages (0.4.1)\n",
      "Requirement already satisfied: accelerate in /home/jupyter/.local/lib/python3.10/site-packages (0.23.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/jupyter/.local/lib/python3.10/site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /kernel/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/jupyter/.local/lib/python3.10/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/jupyter/.local/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: xxhash in /kernel/fallback/lib/python3.10/site-packages (from datasets) (2.0.0)\n",
      "Requirement already satisfied: multiprocess in /home/jupyter/.local/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /home/jupyter/.local/lib/python3.10/site-packages (from datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: responses<0.19 in /home/jupyter/.local/lib/python3.10/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: psutil in /kernel/lib/python3.10/site-packages (from accelerate) (5.7.3)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /kernel/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /kernel/lib/python3.10/site-packages (from aiohttp->datasets) (3.3.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /kernel/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Collecting urllib3<1.27,>=1.21.1 (from requests->transformers)\n",
      "  Obtaining dependency information for urllib3<1.27,>=1.21.1 from https://files.pythonhosted.org/packages/b0/53/aa91e163dcfd1e5b82d8a890ecf13314e3e149c05270cc644581f77f17fd/urllib3-1.26.18-py2.py3-none-any.whl.metadata\n",
      "  Downloading urllib3-1.26.18-py2.py3-none-any.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m654.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /kernel/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n",
      "Collecting charset-normalizer<4.0,>=2.0 (from aiohttp->datasets)\n",
      "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /kernel/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /kernel/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.25.2)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /kernel/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /kernel/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /kernel/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: urllib3, charset-normalizer\n",
      "\u001b[33m  WARNING: The script normalizer is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "boto3 1.20.24 requires botocore<1.24.0,>=1.23.24, but you have botocore 1.31.64 which is incompatible.\n",
      "cloud-ml 0.0.1 requires s3fs<=0.5.2,>=0.4.1, but you have s3fs 2023.10.0 which is incompatible.\n",
      "gcsfs 2023.6.0 requires fsspec==2023.6.0, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed charset-normalizer-2.0.12 urllib3-1.26.18\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers tokenizers datasets evaluate accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c343afb",
   "metadata": {
    "cellId": "ksyl1g4026iecxq8gx3y7i",
    "execution_id": "1ca722bd-54d0-49cb-ba0f-f31785d40372"
   },
   "source": [
    "В текущем варианте при работе в DataSphere возникают проблемы при использовании файлового хранилища. Для решения проблем нам нужно установить последнюю версию библиотеки `s3fs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a581a6a",
   "metadata": {
    "cellId": "ung9q52gkel6fodtt5ncyx",
    "execution": {
     "iopub.execute_input": "2023-11-22T13:06:22.446953Z",
     "iopub.status.busy": "2023-11-22T13:06:22.446097Z",
     "iopub.status.idle": "2023-11-22T13:06:34.003628Z",
     "shell.execute_reply": "2023-11-22T13:06:34.002501Z",
     "shell.execute_reply.started": "2023-11-22T13:06:22.446896Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/dask/s3fs\n",
      "  Cloning https://github.com/dask/s3fs to /tmp/pip-req-build-0m72qmi8\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/dask/s3fs /tmp/pip-req-build-0m72qmi8\n",
      "  Resolved https://github.com/dask/s3fs to commit 949442693ec940b35cda3420c17a864fbe426567\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: aiobotocore~=2.7.0 in /home/jupyter/.local/lib/python3.10/site-packages (from s3fs==2023.10.0+1.g9494426) (2.7.0)\n",
      "Requirement already satisfied: fsspec==2023.10.0 in /home/jupyter/.local/lib/python3.10/site-packages (from s3fs==2023.10.0+1.g9494426) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from s3fs==2023.10.0+1.g9494426) (3.8.5)\n",
      "Requirement already satisfied: botocore<1.31.65,>=1.31.16 in /home/jupyter/.local/lib/python3.10/site-packages (from aiobotocore~=2.7.0->s3fs==2023.10.0+1.g9494426) (1.31.64)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /kernel/lib/python3.10/site-packages (from aiobotocore~=2.7.0->s3fs==2023.10.0+1.g9494426) (1.14.1)\n",
      "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from aiobotocore~=2.7.0->s3fs==2023.10.0+1.g9494426) (0.11.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /kernel/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2023.10.0+1.g9494426) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /kernel/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2023.10.0+1.g9494426) (3.3.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2023.10.0+1.g9494426) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2023.10.0+1.g9494426) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2023.10.0+1.g9494426) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2023.10.0+1.g9494426) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2023.10.0+1.g9494426) (1.3.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.31.65,>=1.31.16->aiobotocore~=2.7.0->s3fs==2023.10.0+1.g9494426) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /kernel/lib/python3.10/site-packages (from botocore<1.31.65,>=1.31.16->aiobotocore~=2.7.0->s3fs==2023.10.0+1.g9494426) (2.8.2)\n",
      "Requirement already satisfied: urllib3<2.1,>=1.25.4 in /kernel/lib/python3.10/site-packages (from botocore<1.31.65,>=1.31.16->aiobotocore~=2.7.0->s3fs==2023.10.0+1.g9494426) (2.0.7)\n",
      "Requirement already satisfied: idna>=2.0 in /kernel/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2023.10.0+1.g9494426) (3.4)\n",
      "Requirement already satisfied: six>=1.5 in /kernel/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.31.65,>=1.31.16->aiobotocore~=2.7.0->s3fs==2023.10.0+1.g9494426) (1.16.0)\n",
      "Building wheels for collected packages: s3fs\n",
      "  Building wheel for s3fs (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for s3fs: filename=s3fs-2023.10.0+1.g9494426-py3-none-any.whl size=29028 sha256=99675eb4bca76f8faea55adc985e86d194007f67b3480726596eac0aaf69730f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ho9nqgih/wheels/1c/2c/ca/bd47ad7042338ff00f37a24b19e98e1374da8e22b72baf14a6\n",
      "Successfully built s3fs\n",
      "Installing collected packages: s3fs\n",
      "  Attempting uninstall: s3fs\n",
      "    Found existing installation: s3fs 2023.10.0\n",
      "    Uninstalling s3fs-2023.10.0:\n",
      "      Successfully uninstalled s3fs-2023.10.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cloud-ml 0.0.1 requires s3fs<=0.5.2,>=0.4.1, but you have s3fs 2023.10.0+1.g9494426 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed s3fs-2023.10.0+1.g9494426\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade git+https://github.com/dask/s3fs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3f15e8",
   "metadata": {
    "cellId": "26xvyymzgf9rbljkfquk3",
    "execution_id": "ed5b5cd9-8b09-47c2-aefc-76c2e1b11867"
   },
   "source": [
    "### Подготовка датасета\n",
    "\n",
    "В нашем примере, мы будем обучать виртуального Льва Толстого. Для этого, возьмём все основные романы писателя, и подготовим их них датасет. В качестве отправной точки будет использовать тексты из [библиотеки Мошкова](http://lib.ru). Соберем ссылки на романы Анна Каренина, Война и Мир и др. в один список:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44d7ffcc",
   "metadata": {
    "cellId": "iclg75lmp4eq0s1e06tgz8",
    "execution": {
     "iopub.execute_input": "2023-11-22T13:06:34.006420Z",
     "iopub.status.busy": "2023-11-22T13:06:34.005468Z",
     "iopub.status.idle": "2023-11-22T13:06:34.023538Z",
     "shell.execute_reply": "2023-11-22T13:06:34.022313Z",
     "shell.execute_reply.started": "2023-11-22T13:06:34.006372Z"
    }
   },
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"http://az.lib.ru/t/tolstoj_lew_nikolaewich/text_0039.shtml\",\n",
    "    \"http://az.lib.ru/t/tolstoj_lew_nikolaewich/text_0040.shtml\",\n",
    "    \"http://az.lib.ru/t/tolstoj_lew_nikolaewich/text_0050.shtml\",\n",
    "    \"http://az.lib.ru/t/tolstoj_lew_nikolaewich/text_0060.shtml\",\n",
    "    \"http://az.lib.ru/t/tolstoj_lew_nikolaewich/text_0070.shtml\",\n",
    "    \"http://az.lib.ru/t/tolstoj_lew_nikolaewich/text_0080.shtml\",\n",
    "    \"http://az.lib.ru/t/tolstoj_lew_nikolaewich/text_0090.shtml\",\n",
    "    \"http://az.lib.ru/t/tolstoj_lew_nikolaewich/text_1860_dekabristy.shtml\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305027b2",
   "metadata": {
    "cellId": "tckha1m6ce8l6uzswytdo",
    "execution_id": "79951af7-53d9-4ec2-aa1c-c6fb34818c24"
   },
   "source": [
    "Теперь скачаем все материалы и подготовим из них один большой текстовый файл. Для того нам понадобится убрать HTML-теги, а также несколько первоначальных строчек в каждом из файлов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84ed7313",
   "metadata": {
    "cellId": "nnln4ab6z2rmu8k09ydl",
    "execution": {
     "iopub.execute_input": "2023-11-22T13:06:37.190694Z",
     "iopub.status.busy": "2023-11-22T13:06:37.189627Z",
     "iopub.status.idle": "2023-11-22T13:06:38.738412Z",
     "shell.execute_reply": "2023-11-22T13:06:38.737280Z",
     "shell.execute_reply.started": "2023-11-22T13:06:37.190658Z"
    }
   },
   "outputs": [],
   "source": [
    "import html\n",
    "import re\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "def download(url):\n",
    "    return requests.get(url).text\n",
    "\n",
    "\n",
    "# code borrowed from here: https://github.com/pallets/markupsafe/blob/0.23/markupsafe/__init__.py#L21\n",
    "striptags_re = re.compile(r\"(<!--.*?-->|<[^>]*>)\")\n",
    "entity_re = re.compile(r\"&([^;]+);\")\n",
    "\n",
    "\n",
    "def to_text(s):\n",
    "    return html.unescape(striptags_re.sub(\"\", s))\n",
    "\n",
    "\n",
    "def beautify(s):\n",
    "    lines = [x.strip() for x in s.split(\"\\n\") if x.strip() != \"\"]\n",
    "    for i in range(min(100, len(lines))):\n",
    "        if lines[i] == \"-->\":\n",
    "            break\n",
    "    return \"\\n\".join(lines[i + 1 :] if i < 100 else lines)\n",
    "\n",
    "\n",
    "with open(\"dataset.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for u in urls:\n",
    "        text = beautify(to_text(download(u)))\n",
    "        f.write(text + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dd8e6a",
   "metadata": {
    "cellId": "4djmsjyt9wpdh9l8h9j3l",
    "execution_id": "24bf09e3-9b1f-47bd-940a-f59f19e01073"
   },
   "source": [
    "В результате мы получили один большой файл `dataset.txt`, содержащий большой корпус текстов Льва Толстого.\n",
    "\n",
    "### Датасеты в Yandex DataSphere\n",
    "\n",
    "При использовании Yandex DataSphere, у нас ограничен объем данных, которые мы можем хранить вместе с проектом. Обычно, большие объемы данных в облаке хранят в **объектном хранилище S3**. DataSphere позволяет легко подключаться к таким хранилищам, монтируя их как обычную директорию в проекте, после чего можно получить доступ к данным как к обычным файлам.\n",
    "\n",
    "Однако, доступ в хранилище S3 не слишком быстрый, а для обучения сетей хочется отдавать данные как можно быстрее, не тормозя вычислительный процесс. Для этого в DataSphere предусмотрены **датасеты** - это отдельные виртуальные накопители, которые можно легко подключать к различным вычислительным ресурсам.\n",
    "\n",
    "Будучи созданным, датасет не может быть изменён - это обеспечивает сохранность исходных данных. Хорошим стилем считается хранить все данные для обучения моделей в датасетах. Кроме того, датасеты можно разделять между другими участниками сообщества или проекта.\n",
    "\n",
    "В нашем случае объем обучающих данных небольшой, и можно обойтись без создания датасета. Но если вы хотите попробовать - добавьте ниже ячейку со следующим кодом и запустите его:\n",
    "```\n",
    "#!:bash\n",
    "#pragma dataset init mytext --size 1Gb\n",
    "cp dataset.txt /home/jupyter/mnt/datasets/mytext\n",
    "```\n",
    "Это создаст датасет `mytext` с единственным файлом `dataset.txt`. При этом ниже в коде вам нужно будет изменить путь к файлу `dataset.txt` на `/home/jupyter/mnt/datasets/mytext/dataset.txt`.\n",
    "\n",
    "> Кажется, что в создании датасета нет большого смысла, поскольку мы просто положили тот же файл в другое место. На самом деле это не так - теперь файл `dataset.txt` не будет занимать место в хранилище проекта, доступ к нему будет быстрее, а также вы сможете легко поделиться датасетом с другими участниками команды, чтобы им не пришлось писать код по предварительной обработке данных. При этом датасет не будет копироваться, а будет просто смонтирован в соответствующие директории в DataSphere."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b835e2",
   "metadata": {
    "cellId": "nunyvossq6o0yteon40rkk",
    "execution_id": "1b9df5be-83bf-4505-ae10-7fcd613f2ecf"
   },
   "source": [
    "### Токенизация\n",
    "\n",
    "Нейросети работают с числами, поэтому первым этапом является токенизация текста, т.е. разбиение его на атомарные элементы, которые затем можно добавить в словарь, и представлять текст как последовательность индексов в словаре. Текст можно токенизировать по буквам, или по словам.\n",
    "\n",
    "При построении современных генеративных сетей текст обычно разбивают на фрагменты таким образом, чтобы частота появления каждого фрагмента в тексте была примерно одинакова. Это лежит в основе т.н. Byte-Pair Encoding (BPE). Подробнее можно прочитать [в этой статье](https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt).\n",
    "\n",
    "Для обучения своего токенизатора используем библиотеку `tokenizers`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d25b461",
   "metadata": {
    "cellId": "x7fii1yudivznf9h1jgxb",
    "execution": {
     "iopub.execute_input": "2023-11-22T13:06:44.265151Z",
     "iopub.status.busy": "2023-11-22T13:06:44.264232Z",
     "iopub.status.idle": "2023-11-22T13:06:47.935533Z",
     "shell.execute_reply": "2023-11-22T13:06:47.934344Z",
     "shell.execute_reply.started": "2023-11-22T13:06:44.265075Z"
    }
   },
   "outputs": [],
   "source": [
    "import tokenizers as tok\n",
    "import transformers as tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e470b754",
   "metadata": {
    "cellId": "jjhwu4gsmle8v2ojdyjukv",
    "execution": {
     "iopub.execute_input": "2023-11-22T13:06:53.487505Z",
     "iopub.status.busy": "2023-11-22T13:06:53.486439Z",
     "iopub.status.idle": "2023-11-22T13:06:56.051785Z",
     "shell.execute_reply": "2023-11-22T13:06:56.050627Z",
     "shell.execute_reply.started": "2023-11-22T13:06:53.487469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tok.Tokenizer(tok.models.BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = tok.pre_tokenizers.Whitespace()\n",
    "trainer = tok.trainers.BpeTrainer(special_tokens=[\"[PAD]\"])\n",
    "tokenizer.train([\"dataset.txt\"], trainer)\n",
    "tokenizer.enable_padding()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9147ea7e",
   "metadata": {
    "cellId": "elx7gkaggq7m8oqmajnxh",
    "execution_id": "f07d8caa-a3c4-40d8-b637-f1234bc4f642"
   },
   "source": [
    "А данном случае мы используем два специальных токена - `[UNK]` для представления неизвестного токена (такое случится, если на вход попадёт символ, который токенизатор не видел при обучении), и `[PAD]` для **паддинга** - он используется, если нужно дополнить последовательность до определённой длины.\n",
    "\n",
    "Вот как можно закодировать входной текст:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04579048",
   "metadata": {
    "cellId": "d1cjx8u0oy8rixk9y6m8q",
    "execution": {
     "iopub.execute_input": "2023-11-22T13:07:08.891474Z",
     "iopub.status.busy": "2023-11-22T13:07:08.890755Z",
     "iopub.status.idle": "2023-11-22T13:07:08.922664Z",
     "shell.execute_reply": "2023-11-22T13:07:08.921593Z",
     "shell.execute_reply.started": "2023-11-22T13:07:08.891438Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Иван',\n",
       " 'С',\n",
       " 'иг',\n",
       " 'изму',\n",
       " 'н',\n",
       " 'до',\n",
       " 'вич',\n",
       " 'подошел',\n",
       " 'к',\n",
       " 'окну',\n",
       " 'и',\n",
       " 'закашлялся',\n",
       " '.',\n",
       " 'Вечер',\n",
       " 'ело',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"Иван Сигизмундович подошел к окну и закашлялся. Вечерело.\").tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156f66e2",
   "metadata": {
    "cellId": "kxbtrltl0yra2oywv2mxe"
   },
   "source": [
    "Видим, что популярные слова токенизируются целиком, а те, которые встречаются в тексте редко или не встречаются вовсе - разбиваются на фрагменты.\n",
    "\n",
    "### Генеративные трансформеры\n",
    "\n",
    "Для генерации текста используются архитектуры GPT - Generative Pre-trained Transformers. В то время как полноценные трансформеры являются энкодер-декодерной архитектурой, т.е. могут решать задачи преобразования одного вида последовательности в другую, GPT является только декодером, т.к. способно прогнозировать распределение вероятности следующего слова по начальной части последовательности.\n",
    "\n",
    "Мы используем архитектуру GPT-2, которая, с одной стороны, не слишком огромна, а с другой - может неплохо обучиться. Сперва попробуем натренировать такую архитетуру \"с нуля\".\n",
    "\n",
    "Дла начала нам потребуется преобразовать наш токенизатор к объекту `ttokenizer`, который понимает библиотека transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8928e282",
   "metadata": {
    "cellId": "76f33tr549x3izkn7g8t7w",
    "execution": {
     "iopub.execute_input": "2023-11-22T13:07:20.166309Z",
     "iopub.status.busy": "2023-11-22T13:07:20.165368Z",
     "iopub.status.idle": "2023-11-22T13:07:20.306703Z",
     "shell.execute_reply": "2023-11-22T13:07:20.305635Z",
     "shell.execute_reply.started": "2023-11-22T13:07:20.166270Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = tokenizer.get_vocab()\n",
    "ttokenizer = tr.PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91488224-b24c-44d6-a805-b4cea7ad833a",
   "metadata": {},
   "source": [
    "Теперь создадим непосредственно нейросетевую модель GPT2. При этом основные параметры (количество слоёв, количество голов внимания и т.д. оставим по умолчанию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b18d8be",
   "metadata": {
    "cellId": "7yflw4cbp4mapc77dbn4o",
    "execution": {
     "iopub.execute_input": "2023-11-22T13:07:21.219041Z",
     "iopub.status.busy": "2023-11-22T13:07:21.218320Z",
     "iopub.status.idle": "2023-11-22T13:07:37.475252Z",
     "shell.execute_reply": "2023-11-22T13:07:37.474143Z",
     "shell.execute_reply.started": "2023-11-22T13:07:21.219005Z"
    }
   },
   "outputs": [],
   "source": [
    "config = tr.GPT2Config(\n",
    "    vocab_size=len(vocab),\n",
    "    bos_token_id=tokenizer.token_to_id(\"[CLS]\"),\n",
    "    eos_token_id=tokenizer.token_to_id(\"[EOS]\"),\n",
    ")\n",
    "gpt = tr.GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8379893b-f03d-48cc-b09f-7c085b15f0ce",
   "metadata": {},
   "source": [
    "Веса вновь созданной модели инициализируются случайным образом, поэтому если мы попросим такую модель сгенерировать текст - получится бессмыслица:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9eec498e",
   "metadata": {
    "cellId": "pd21krza2dmw3ecnel8ara",
    "execution": {
     "iopub.execute_input": "2023-11-22T13:07:37.478365Z",
     "iopub.status.busy": "2023-11-22T13:07:37.477330Z",
     "iopub.status.idle": "2023-11-22T13:07:39.412338Z",
     "shell.execute_reply": "2023-11-22T13:07:39.411260Z",
     "shell.execute_reply.started": "2023-11-22T13:07:37.478319Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Мне нравится прине прине могут грабить pa pa pa грабить Пелагеюшка огляды en en en en свяще свяще худая худая говорит Север грабить камня камня камня свободны свободны приводить худая говорит говорит!...\" присудили присудили Бонапарта Бонапарта Бонапарта камня камня тобой новиз занимали занимали грабить дворо свяще свяще свяще свяще навсегда навсегда'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = gpt.generate(\n",
    "    **ttokenizer(\"Мне нравится \", return_tensors=\"pt\"),\n",
    "    max_new_tokens=50,\n",
    "    top_k=3,\n",
    "    do_sample=True\n",
    ")\n",
    "ttokenizer.decode(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4bd5a4-3378-4a38-ac3d-07f48ad00047",
   "metadata": {},
   "source": [
    "Теперь нам надо научиться подавать на вход модели фрагменты текста для обучения. Для этого существует библиотека `datasets`, входящее в семейство трансформерных библиотек HuggingFace. Помимо того, что эта библиотека умеет работать с разными форматами входных датасетов, она также интегрирована с HuggingFace Hub, и может в одну строчку загружать множество имеющихся на этом сайте датасетов.\n",
    "\n",
    "В нашем случае мы загрузим датасет из текстового файла:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04f83e7a",
   "metadata": {
    "cellId": "ymos6h01o5efyusa8fmapr",
    "execution": {
     "iopub.execute_input": "2023-11-22T13:07:51.422222Z",
     "iopub.status.busy": "2023-11-22T13:07:51.421328Z",
     "iopub.status.idle": "2023-11-22T13:07:53.369455Z",
     "shell.execute_reply": "2023-11-22T13:07:53.368323Z",
     "shell.execute_reply.started": "2023-11-22T13:07:51.422187Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "414671319b01493d82df985e0835966a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "847cb197ed2a46888c983bb582e29c13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98369d29d5b1400ba9c08bff712849aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'Он взял своею большою рукой меня за руку, и пожал так крепко, честно, только что не больно. Я думала, что он поцелует мою руку, и нагнулась было к нему, но он еще раз пожал мне руку и прямо в глаза посмотрел своим твердым и веселым взглядом.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "dataset = datasets.load_dataset(\"text\", data_files=\"dataset.txt\")\n",
    "dataset[\"train\"][13]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c042844-9998-4a05-bc69-165b77896fcf",
   "metadata": {},
   "source": [
    "Далее нам необходимо научиться токенизировать датасет, т.е. преобразовывать в числовые тензоры, которые затем мы будем подавать на вход нейросети в процессе обучения. Для этого опишем фукнцию `tokenize`, которая будет возвращать словарь с несколькими полями:\n",
    "\n",
    "* `input_ids` - это собственно номера слов входной последовательности в словаре\n",
    "* `token_type_ids` - содержит нули. Это поле используется в более сложных сценариях, например, когда мы тренируем сеть отвечать на вопросы по тексту. В этом случае нам нужно подать на вход текст + вопрос, и это поле позволяет различать между несколькими разными по смыслу фрагментами входной последовательности\n",
    "* `atttention_mask` показывает, какая часть входной последовательности значима. Для организации последовательности в minibatch нам может потребоваться дополнить последовательность до максимальной длины, и поле `attention_mask` содержит 1 в тех позициях, которые соответствуют исходной последовательности\n",
    "\n",
    "Такой формат входных данных типичен для трансформерной архитектуры. Также мы передаем последовательность значений целевой переменной `labels`, но поскольку наша задача - это генерация текста, то в качестве `labels` мы передаём копию исходного текста. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72f7b2ad",
   "metadata": {
    "cellId": "svhzelwehjpwlu9emfsib",
    "execution": {
     "iopub.execute_input": "2023-11-22T13:07:59.902663Z",
     "iopub.status.busy": "2023-11-22T13:07:59.901666Z",
     "iopub.status.idle": "2023-11-22T13:08:03.142172Z",
     "shell.execute_reply": "2023-11-22T13:08:03.140784Z",
     "shell.execute_reply.started": "2023-11-22T13:07:59.902606Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0daa860af7ae484dab9072888b1adf55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [9585, 9735, 3192],\n",
       " 'token_type_ids': [0, 0, 0],\n",
       " 'attention_mask': [1, 1, 1],\n",
       " 'labels': [9585, 9735, 3192]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(x):\n",
    "    x = ttokenizer(x[\"text\"])\n",
    "    x[\"labels\"] = x[\"input_ids\"].copy()\n",
    "    return x\n",
    "\n",
    "\n",
    "ds = dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "ds[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5800b193-896c-4b68-97cf-584cd69e1880",
   "metadata": {},
   "source": [
    "Для обучения лучше всего использовать длинные фрагменты текста, поэтому мы сгруппируем все последовательности токенов в блоки размером `block_size`. Для этого мы сначала сконкатенируем все последовательности, а потом разобъем их на блоки. В данном случае мы не будем даже разбивать последовательность на слова и/или предложения - как показывает практика, такой упрощенный подход также даёт хорошие результаты.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6b55b96",
   "metadata": {
    "cellId": "omlv6mxp3tqt8ebm0awq",
    "execution": {
     "iopub.execute_input": "2023-11-22T14:34:15.997454Z",
     "iopub.status.busy": "2023-11-22T14:34:15.996759Z",
     "iopub.status.idle": "2023-11-22T14:34:18.603876Z",
     "shell.execute_reply": "2023-11-22T14:34:18.602744Z",
     "shell.execute_reply.started": "2023-11-22T14:34:15.997410Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a0db1f11e54dbe9ee874d81447261e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "block_size = 1024\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "dsb = ds.map(group_texts, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd56ffd1-995d-44e0-89d6-7740a0f075b6",
   "metadata": {},
   "source": [
    "Теперь мы готовы к обучению! Для задания параметров обучения мы создаём объект `TrainingArguments`, в котором задаем директорию, куда будут записываться промежуточные результаты обучения, число эпох, скорость обучения и т.д. Затем на основе этих параметров создаём объект `Trainer`.\n",
    "\n",
    "Обратите внимание, что размер записываемой на диск сети GPT-2 может быть весьма большим (около 1.4 Gb), что может привести к исчерпанию размера вашей домашней директории в DataSphere. Исходя из этого лучше выбирать параметры `save_steps` и `num_train_epochs` таким образом, чтобы количество записываемых на диск чекпоинтов не превышало 3-5 шт.\n",
    "\n",
    "Для начала стоит попробовать пообучать сеть в течение 30-90 минут, чтобы увидеть, что она начинает складывать слова более менее правдоподобно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3408488f",
   "metadata": {
    "cellId": "4re85sxlr1hkjznsu65qq",
    "execution": {
     "iopub.execute_input": "2023-11-22T13:08:29.022680Z",
     "iopub.status.busy": "2023-11-22T13:08:29.021433Z",
     "iopub.status.idle": "2023-11-22T13:08:56.689692Z",
     "shell.execute_reply": "2023-11-22T13:08:56.688516Z",
     "shell.execute_reply.started": "2023-11-22T13:08:29.022635Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 13:08:41.233656: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "targs = tr.TrainingArguments(\n",
    "    output_dir=\"gpt2-scratch\",\n",
    "    num_train_epochs=30,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=200,\n",
    "    save_steps=1500,\n",
    ")\n",
    "trainer = tr.Trainer(\n",
    "    gpt,\n",
    "    args=targs,\n",
    "    train_dataset=dsb[\"train\"],\n",
    "    tokenizer=ttokenizer,\n",
    "    data_collator=tr.default_data_collator,  # tr.DataCollatorForLanguageModeling(tokenizer=ttokenizer,mlm=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cee9c3a",
   "metadata": {
    "cellId": "r86o6ai2mfz02zty9jld",
    "execution": {
     "iopub.execute_input": "2023-11-22T13:09:19.335856Z",
     "iopub.status.busy": "2023-11-22T13:09:19.334770Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2011' max='4680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2011/4680 26:27 < 35:09, 1.27 it/s, Epoch 12.88/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>7.210100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>6.113800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>5.736100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>5.436200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcda58f-f57d-42b6-88bd-8316b31816be",
   "metadata": {},
   "source": [
    "Теперь посмотрим, как работает генерация:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b03fba89",
   "metadata": {
    "cellId": "leffxl7khjsji465vkdloo",
    "execution": {
     "iopub.execute_input": "2023-11-22T14:14:51.133759Z",
     "iopub.status.busy": "2023-11-22T14:14:51.132840Z",
     "iopub.status.idle": "2023-11-22T14:14:53.740387Z",
     "shell.execute_reply": "2023-11-22T14:14:53.739172Z",
     "shell.execute_reply.started": "2023-11-22T14:14:51.133700Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Пьер закашлялся и что в Москве был был быть. Но она была у нее не хотелось сказать своей смерти, и он был не знал, с тем, как было видеть этого лица, которое она узнала, что она не желала, что он сам. Она подошла к ней ; я не была рада, как ни о будущем ее, о чем она не понимала этого чувства не могла понять, но не знать, что она хотела думать об этом не будет и она не могла, как и не говорила о нее, что бы сделать. Она, но ничего не могла понять, как не было не понимала, но все это легко будет у меня и о том, как можно так, а она видела она не могла сделать, как и даже как он не видела, как бы она почувствовала'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ttokenizer = tr.PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n",
    "res = gpt.generate(\n",
    "    **ttokenizer(\"Пьер закашлялся и\", return_tensors=\"pt\").to(\"cuda\"),\n",
    "    max_new_tokens=150,\n",
    "    do_sample=True\n",
    ")\n",
    "ttokenizer.decode(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674d8ea2-4d80-4c55-b2ae-ba94873d3483",
   "metadata": {},
   "source": [
    "Кажется, что сгенерированный текст пока ещё не слишком осмысленный. Но сравните его с первоначальным текстом, сгенерированным необученной нейросетью - в нём почти не было корректных грамматических конструкций. За примерно час обучения сеть уже стала неплохо понимать, какие слова хорошо сочетаются друг с другом, и в целом начала говорить более осмысленно. Помните, что трансформерная модель - сложная, и для обучения полноценной GPT-2 \"с нуля\" требуются сотни и тысячи GPU-часов.\n",
    "\n",
    "> Прежде, чем переходить к следующим экспериментам, очистим память. Если вдруг на следующем этапе возникнет переполнение памяти GPU, может потребоваться перезапуск ядра ноутбука - выберите к меню Kernel -> Restart Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5903b42d",
   "metadata": {
    "cellId": "c5wv13vypc4shk1wncy5cm",
    "execution": {
     "iopub.execute_input": "2023-11-22T14:16:34.824292Z",
     "iopub.status.busy": "2023-11-22T14:16:34.823428Z",
     "iopub.status.idle": "2023-11-22T14:16:35.140660Z",
     "shell.execute_reply": "2023-11-22T14:16:35.139479Z",
     "shell.execute_reply.started": "2023-11-22T14:16:34.824254Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "gpt = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70215e43-c58e-4182-bda1-726a0b5c8939",
   "metadata": {},
   "source": [
    "## До-обучение GPT-2\n",
    "\n",
    "За приемлемое время сложно достичь приемлемого качества обучения трансформера, поэтому обычно используют предобученные модели (поэтому в названии GPT и фигурирует слово *Pretrained*), которые уже научились \"читать\" на нужном языке, и их необходимо лишь немного \"доучить\" под требуемую предметную область или стиль. В этом случае процесс обучения модели почти не отличается от того, что мы делали ранее - с той лишь разницей, что необходимо использовать токенизатор, который использовался при обучении исходной модели.\n",
    "\n",
    "Для начала, загрузим предобученную модель **ruGPT** и соответствующий токенизатор, и посмотрим, как эта модель умеет продолжать текст:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bafd8bbd-77e9-442a-a36a-3d1883568f83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T14:38:05.664573Z",
     "iopub.status.busy": "2023-11-22T14:38:05.663434Z",
     "iopub.status.idle": "2023-11-22T14:38:29.085092Z",
     "shell.execute_reply": "2023-11-22T14:38:29.083780Z",
     "shell.execute_reply.started": "2023-11-22T14:38:05.664535Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "804f6bce89204af090408c175fe13b18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3386569b8204b9eb7f19fb2e9fa1bd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/1.71M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97d9c578fdd44553837c24568b8f774a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/1.27M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51797b9af4b340b5b6ab1844261f2004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/551M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Мне нравится, что вы \\nне знаете, кто вы и что вы.\\n\\n- Я знаю только, кто вы, - сказала она. - Вы -\\n\\nнесколько человек.\\n\\n- Вы -\\n\\nне совсем\\n\\nчеловек.\\n\\n- Я'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = tr.AutoTokenizer.from_pretrained(\"ai-forever/rugpt3small_based_on_gpt2\")\n",
    "gpt = tr.GPT2LMHeadModel.from_pretrained(\"ai-forever/rugpt3small_based_on_gpt2\")\n",
    "res = gpt.generate(\n",
    "    **tokenizer(\"Мне нравится, что вы \", return_tensors=\"pt\"),\n",
    "    max_new_tokens=50,\n",
    "    top_k=3,\n",
    "    do_sample=True\n",
    ")\n",
    "tokenizer.decode(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616cda96-ac15-49fe-ad2c-d7762cf9bf8d",
   "metadata": {},
   "source": [
    "На самом деле качество модели *очень сильно* зависит от количества параметров, и тот факт, что мы взяли модель **ruGPTsmall** сказывается на качестве текста. Но зато и процесс обучения будет существенно быстрее!\n",
    "\n",
    "Поскольку мы теперь используем другой токенизатор, то нам нужно заново токенизировать датасет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66a6b616-0bb8-4ade-89c6-846643699ac0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T14:39:23.217520Z",
     "iopub.status.busy": "2023-11-22T14:39:23.216267Z",
     "iopub.status.idle": "2023-11-22T14:39:26.558165Z",
     "shell.execute_reply": "2023-11-22T14:39:26.556877Z",
     "shell.execute_reply.started": "2023-11-22T14:39:23.217453Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03c50add30d84007b56bb75d5e3f7dee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"text\", data_files=\"dataset.txt\")\n",
    "ds = dataset.map(lambda x: \n",
    "                 tokenizer(x[\"text\"]), batched=True, remove_columns=[\"text\"])\n",
    "dsb = ds.map(group_texts, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f324969-e2ed-4085-862d-906ab5bf9777",
   "metadata": {},
   "source": [
    "Сам по себе процесс запуска обучения и указания параметров ничем не отличается от обучения трансформерной модели \"с нуля\". Возможно, при до-обучении имеет смысл указывать чуть более низкий `learning_rate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ced1c5f-8a27-47c0-9dd4-276f5cd33f2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T14:39:31.920477Z",
     "iopub.status.busy": "2023-11-22T14:39:31.919469Z",
     "iopub.status.idle": "2023-11-22T16:02:29.020689Z",
     "shell.execute_reply": "2023-11-22T16:02:29.019128Z",
     "shell.execute_reply.started": "2023-11-22T14:39:31.920441Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 14:39:34.067839: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5700' max='5700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5700/5700 1:22:50, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.277100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.917700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.686700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.496100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.348400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.217000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.112400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.029200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.957200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.907200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.872600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5700, training_loss=2.3302585534882128, metrics={'train_runtime': 4971.7497, 'train_samples_per_second': 9.154, 'train_steps_per_second': 1.146, 'total_flos': 2.378280075264e+16, 'train_loss': 2.3302585534882128, 'epoch': 30.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targs = tr.TrainingArguments(\n",
    "    output_dir=\"gpt2-finetune\",\n",
    "    num_train_epochs=30,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=200,\n",
    "    save_steps=1500,\n",
    ")\n",
    "trainer = tr.Trainer(\n",
    "    gpt,\n",
    "    args=targs,\n",
    "    train_dataset=dsb[\"train\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=tr.default_data_collator,  # tr.DataCollatorForLanguageModeling(tokenizer=ttokenizer,mlm=False)\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc9e6b4-eee1-4071-9aa1-a4b5a96a1c0a",
   "metadata": {},
   "source": [
    "Смотрим на результат генерации после обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "548b3d10-d230-4e55-a6ae-645630603dac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T16:14:15.085816Z",
     "iopub.status.busy": "2023-11-22T16:14:15.084658Z",
     "iopub.status.idle": "2023-11-22T16:14:16.067040Z",
     "shell.execute_reply": "2023-11-22T16:14:16.065953Z",
     "shell.execute_reply.started": "2023-11-22T16:14:15.085770Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Мне нравится, что вы  знаете  это, и я люблю это, -- сказал он, -- и  надеюсь  на  вас.  Вы  непротивоположная, я всегда был и буду  тем и другим,  я  всегдапротивоположный.-- '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = gpt.generate(\n",
    "    **tokenizer(\"Мне нравится, что вы \", return_tensors=\"pt\").to(\"cuda\"),\n",
    "    max_new_tokens=50,\n",
    "    top_k=3,\n",
    "    do_sample=True\n",
    ")\n",
    "tokenizer.decode(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bdac8e-d3db-459f-a864-6c693a642a2e",
   "metadata": {},
   "source": [
    "Кажется, что мы получили сильно более хороший результат!\n",
    "\n",
    "## Параллелизация обучения\n",
    "\n",
    "Надеюсь, вы убедились, что на DataSphere можно обучать достаточно мощные модели, однако время, затрачиваемое на обучение, всё ещё остаётся большим. Чтобы ускорить этот процесс, обычно используют параллельное обучение на нескольких GPU одновременно.\n",
    "\n",
    "Самым распространённым вариантом параллелизма является параллелизм по данным (Data Parallel Training), в котором на каждый из обучающих GPU подаётся свой поток данных (т.е. своя часть исходного датасета). При этом на каждом обучающем шаге каждый GPU вычисляет свой градиент ошибки, которые затем усредняются и используются для синхронного обновления моделей на всех обучающих процессорах.\n",
    "\n",
    "Различают два варианта обучения на нескольких GPU:\n",
    "* **Data Parallel** - обычно используется, когда несколько GPU установлены на одном компьютере. В этом случае используется почти такой же код обучения на Python, как для однопроцессорного варианта, модель оборачивается в класс `torch.nn.DataParallel`, и минибатч распределяется по нескольким доступным на данном компьютере GPU.\n",
    "* **Distributed Data Parallel** используется в более общем случае, когда есть кластер из компьютеров с GPU.\n",
    "\n",
    "Подробнее про параллельное обучения можно почитать [в руководстве PyTorch](https://pytorch.org/docs/stable/distributed.html#distributed-basics).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179c51b4-ae57-4c1a-997e-5d71eb820256",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T14:30:02.748101Z",
     "iopub.status.busy": "2023-11-22T14:30:02.747033Z",
     "iopub.status.idle": "2023-11-22T14:30:02.875407Z",
     "shell.execute_reply": "2023-11-22T14:30:02.874294Z",
     "shell.execute_reply.started": "2023-11-22T14:30:02.747981Z"
    },
    "tags": []
   },
   "source": [
    "## Заключение\n",
    "\n",
    "Одна из целей данной работы заключалась в том, чтобы продемонстрировать, что обучение сложных языковых моделей с помощью современных библиотек является сравнительно простой задачей - но требующей значительных вычислительных ресурсов. Как только мы выходим за рамки вычислений, которые можно сделать за несколько часов на общедоступных инструментах типа Google Colab - у нас возникает потребность в облачных вычислительных ресурсах.\n",
    "\n",
    "Yandex DataSphere обеспечивает легкий переход от локального Jupyter Notebook или публичного облака Google Colab / Kaggle к выделенной облачной инфраструктуре в Yandex Cloud. В DataSphere вы можете:\n",
    "\n",
    "* легко настроить подключения к облачным хранилищам данных, \n",
    "* взаимодействовать с другими участниками проекта\n",
    "* использовать GitHub для контроля версий кода\n",
    "* бережливо расходовать ресурсы благодаря режиму Serverless или возможности легкого переключения между виртуальными вычислителями\n",
    "\n",
    "Для эффективной работы в DataSphere в ней необходимо немного привыкнуть, но когда этап привыкания пройдёт - вы сможете эффективно пользоваться этим инструментом и получать удовольствие от работы в нём!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5829425e-b2bb-496c-85f0-5cbe8805a798",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "1481bb9f-dbbc-4e48-8133-aa84fa48d93b",
  "notebookPath": "sda-homeworks/train-trans/train-trans.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
